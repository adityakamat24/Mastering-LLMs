{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KV **Cache**"
      ],
      "metadata": {
        "id": "zWESRGSVxr4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
        "\n",
        "# ─── CONFIG ─────────────────────────────────────────────────────────────────────\n",
        "MODEL_NAME = \"gpt2\"\n",
        "PROMPT     = \"The quick brown fox jumps over the lazy dog\"\n",
        "NUM_TOKENS = 500\n",
        "\n",
        "TEMPERATURE = 0.7\n",
        "TOP_K = 50\n",
        "TOP_P = 0.9\n",
        "REPETITION_PENALTY = 1.2\n",
        "\n",
        "device    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)\n",
        "model     = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(device)\n",
        "model.eval()\n",
        "\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "input_ids = tokenizer(PROMPT, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "def sample_token(logits, temperature=TEMPERATURE, top_k=TOP_K, top_p=TOP_P, repetition_penalty=REPETITION_PENALTY):\n",
        "\n",
        "    if repetition_penalty != 1.0:\n",
        "        logits = logits / repetition_penalty\n",
        "\n",
        "    # Apply temperature\n",
        "    if temperature != 1.0:\n",
        "        logits = logits / temperature\n",
        "\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    if top_k is not None:\n",
        "        top_probs, top_indices = torch.topk(probs, k=top_k)\n",
        "        probs = torch.zeros_like(probs).scatter_(-1, top_indices, top_probs)\n",
        "\n",
        "    if top_p is not None:\n",
        "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        probs[:, indices_to_remove] = 0\n",
        "\n",
        "    probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "    next_token = torch.multinomial(probs, num_samples=1)\n",
        "    return next_token\n",
        "\n",
        "with torch.no_grad():\n",
        "    _ = model(input_ids)\n",
        "\n",
        "def time_generation_with_cache():\n",
        "    \"\"\"Efficient generation using KV cache with sampling\"\"\"\n",
        "    generated = input_ids.clone()\n",
        "    past_key_values = None\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(NUM_TOKENS):\n",
        "            input_to_model = generated if past_key_values is None else generated[:, -1:]\n",
        "\n",
        "            outputs = model(\n",
        "                input_to_model,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=True,\n",
        "            )\n",
        "            past_key_values = outputs.past_key_values\n",
        "\n",
        "            next_token = sample_token(outputs.logits[:, -1, :])\n",
        "            generated = torch.cat([generated, next_token], dim=1)\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "\n",
        "    text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "    return text, t1 - t0\n",
        "\n",
        "def time_generation_without_cache():\n",
        "    \"\"\"Inefficient generation - reprocesses all tokens each time\"\"\"\n",
        "    generated = input_ids.clone()\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(NUM_TOKENS):\n",
        "            outputs = model(\n",
        "                generated,\n",
        "                use_cache=False,\n",
        "            )\n",
        "\n",
        "            next_token = sample_token(outputs.logits[:, -1, :])\n",
        "            generated = torch.cat([generated, next_token], dim=1)\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "\n",
        "    text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "    return text, t1 - t0\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Generating {NUM_TOKENS} tokens...\")\n",
        "print(f\"Sampling params: temp={TEMPERATURE}, top_k={TOP_K}, top_p={TOP_P}, rep_penalty={REPETITION_PENALTY}\\n\")\n",
        "\n",
        "text_c, t_c = time_generation_with_cache()\n",
        "text_nc, t_nc = time_generation_without_cache()\n",
        "\n",
        "print(\"=== With KV Cache ===\")\n",
        "print(text_c[:200] + \"...\" if len(text_c) > 200 else text_c)\n",
        "print(f\"Time taken: {t_c:.3f} s\")\n",
        "print(f\"Tokens/sec: {NUM_TOKENS/t_c:.1f}\\n\")\n",
        "\n",
        "print(\"=== Without KV Cache ===\")\n",
        "print(text_nc[:200] + \"...\" if len(text_nc) > 200 else text_nc)\n",
        "print(f\"Time taken: {t_nc:.3f} s\")\n",
        "print(f\"Tokens/sec: {NUM_TOKENS/t_nc:.1f}\\n\")\n",
        "\n",
        "if t_c > 0:\n",
        "    print(f\"Speed-up: {t_nc / t_c:.2f}× faster with KV cache\")\n",
        "    print(f\"Time saved: {t_nc - t_c:.3f} seconds\")\n",
        "else:\n",
        "    print(\"Speed-up: ∞ (cache run was instantaneous?)\")\n",
        "\n",
        "print(f\"\\nWithout cache: O(n²) growth - processed {sum(len(input_ids[0]) + i for i in range(NUM_TOKENS))} total tokens\")\n",
        "print(f\"With cache: O(n) growth - processed {len(input_ids[0]) + NUM_TOKENS} total tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dch9qpuaxIQ0",
        "outputId": "890e14a7-93a0-4d1b-d523-1cd985a4bb0e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Generating 500 tokens...\n",
            "Sampling params: temp=0.7, top_k=50, top_p=0.9, rep_penalty=1.2\n",
            "\n",
            "=== With KV Cache ===\n",
            "The quick brown fox jumps over the lazy dog with his other hand, and makes a gesture to let it follow its owner.\n",
            "\n",
            "\n",
            "The fox then quickly begins chasing after the lazy dog.\n",
            "\n",
            "\n",
            "The fox makes a slight effo...\n",
            "Time taken: 4.752 s\n",
            "Tokens/sec: 105.2\n",
            "\n",
            "=== Without KV Cache ===\n",
            "The quick brown fox jumps over the lazy dog to greet him.\n",
            "\n",
            "The lazy dog does the same thing, a smile fades across his face and he can tell he's laughing.\n",
            "\n",
            "\"Hey,\" the lazy dog says, and just then the f...\n",
            "Time taken: 13.410 s\n",
            "Tokens/sec: 37.3\n",
            "\n",
            "Speed-up: 2.82× faster with KV cache\n",
            "Time saved: 8.657 seconds\n",
            "\n",
            "Without cache: O(n²) growth - processed 129250 total tokens\n",
            "With cache: O(n) growth - processed 509 total tokens\n"
          ]
        }
      ]
    }
  ]
}