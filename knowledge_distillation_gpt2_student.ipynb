{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acec1232",
   "metadata": {},
   "source": [
    "# üß† Knowledge Distillation: GPT-2 ‚Üí TinyTransformer\n",
    "This notebook demonstrates how to train a small transformer (student) to mimic GPT-2 (teacher) on a next-token prediction task using knowledge distillation.\n",
    "\n",
    "**Task**: Language Modeling (Next Token Prediction)  \n",
    "**Teacher**: GPT-2  \n",
    "**Student**: Custom 4-layer Transformer  \n",
    "**Dataset**: WikiText-2 (subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731bbc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb3d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb7c039",
   "metadata": {},
   "source": [
    "## üîó Load GPT-2 Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b49de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "teacher = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "teacher.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b1d7d",
   "metadata": {},
   "source": [
    "## üìö Load and Preprocess WikiText-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d2c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "train_texts = [t for t in raw_dataset['train']['text'] if len(t.strip()) > 30][:5000]\n",
    "\n",
    "class WikiDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, seq_len=32):\n",
    "        self.samples = []\n",
    "        for t in texts:\n",
    "            ids = tokenizer(t, return_tensors=\"pt\", truncation=True,\n",
    "                            max_length=seq_len+1, padding=\"max_length\")['input_ids'].squeeze(0)\n",
    "            self.samples.append((ids[:-1], ids[1:]))\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return self.samples[idx]\n",
    "\n",
    "train_ds = WikiDataset(train_texts, tokenizer)\n",
    "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842833e",
   "metadata": {},
   "source": [
    "## üßë‚Äçüéì Define TinyTransformer Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf5aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=4, ff_dim=512):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        encoder = nn.TransformerEncoderLayer(d_model, nhead, ff_dim)\n",
    "        self.transformer = nn.TransformerEncoder(encoder, num_layers)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x).permute(1, 0, 2)\n",
    "        x = self.transformer(x).permute(1, 0, 2)\n",
    "        return self.lm_head(x)\n",
    "\n",
    "student = TinyTransformer(len(tokenizer)).to(device)\n",
    "optimizer = optim.Adam(student.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aca916",
   "metadata": {},
   "source": [
    "## üî• Train Student with Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64bb390",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2.0\n",
    "alpha = 0.7\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, target):\n",
    "    soft = F.kl_div(F.log_softmax(student_logits / T, dim=-1),\n",
    "                    F.softmax(teacher_logits / T, dim=-1), reduction='batchmean') * T * T\n",
    "    hard = F.cross_entropy(student_logits.view(-1, student_logits.size(-1)), target.view(-1))\n",
    "    return alpha * soft + (1 - alpha) * hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc3991",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, kls, entropies, accs = [], [], [], []\n",
    "for epoch in range(10):\n",
    "    total_loss = total_kl = total_ent = total_acc = 0\n",
    "    student.train()\n",
    "    for x, y in tqdm(train_dl, desc=f\"Epoch {epoch+1}\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad(): t_logits = teacher(x).logits\n",
    "        s_logits = student(x)\n",
    "        loss = distillation_loss(s_logits, t_logits, y)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        kl = F.kl_div(F.log_softmax(s_logits / T, dim=-1), F.softmax(t_logits / T, dim=-1), reduction='batchmean').item()\n",
    "        ent = -(F.softmax(s_logits, dim=-1) * F.log_softmax(s_logits, dim=-1)).sum(-1).mean().item()\n",
    "        acc = (s_logits.argmax(dim=-1) == y).float().mean().item()\n",
    "        total_loss += loss.item(); total_kl += kl; total_ent += ent; total_acc += acc\n",
    "    losses.append(total_loss/len(train_dl))\n",
    "    kls.append(total_kl/len(train_dl))\n",
    "    entropies.append(total_ent/len(train_dl))\n",
    "    accs.append(total_acc/len(train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe1ec83",
   "metadata": {},
   "source": [
    "## üìä Visualize Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "for i, (data, title) in enumerate(zip([losses, kls, entropies, accs],\n",
    "   [\"Loss\", \"KL\", \"Entropy\", \"Accuracy\"])):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.plot(data)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d862a305",
   "metadata": {},
   "source": [
    "## üîç Compare Predictions for a Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0bb46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(sentence, token_index):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\").input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        t_logits = teacher(inputs).logits\n",
    "        s_logits = student(inputs)\n",
    "    t_probs = F.softmax(t_logits[0, token_index], dim=-1).cpu().numpy()\n",
    "    s_probs = F.softmax(s_logits[0, token_index], dim=-1).cpu().numpy()\n",
    "    topk = np.argsort(t_probs)[-10:]\n",
    "    labels = [tokenizer.decode([i]) for i in topk]\n",
    "    x = np.arange(len(labels))\n",
    "    plt.bar(x - 0.2, t_probs[topk], 0.4, label='Teacher')\n",
    "    plt.bar(x + 0.2, s_probs[topk], 0.4, label='Student')\n",
    "    plt.xticks(x, labels, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.title(f\"Token {token_index} prediction: '{tokenizer.decode([inputs[0, token_index].item()])}'\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# visualize_prediction(\"The quick brown fox jumps over the lazy\", 5)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
